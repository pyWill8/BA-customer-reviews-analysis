{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pages scraped\n",
      "2 pages scraped\n",
      "3 pages scraped\n",
      "4 pages scraped\n",
      "5 pages scraped\n",
      "6 pages scraped\n",
      "7 pages scraped\n",
      "8 pages scraped\n",
      "9 pages scraped\n",
      "10 pages scraped\n",
      "11 pages scraped\n",
      "12 pages scraped\n",
      "13 pages scraped\n",
      "14 pages scraped\n",
      "15 pages scraped\n",
      "16 pages scraped\n",
      "17 pages scraped\n",
      "18 pages scraped\n",
      "19 pages scraped\n",
      "20 pages scraped\n"
     ]
    }
   ],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, page_number):\n",
    "        # Getting the webpage and parsing it with lxml.\n",
    "        self.the_main_link = \"https://www.airlinequality.com/airline-reviews/british-airways/page/\" + str(page_number) + \"/?sortby=post_date%3ADesc&pagesize=100\"\n",
    "        self.webpage_response = requests.get(self.the_main_link)\n",
    "        self.webpage = self.webpage_response.content\n",
    "        self.webpage = BeautifulSoup(self.webpage, \"lxml\")\n",
    "\n",
    "        # Getting the review stats (made up of nominal and ordinal data (being flight info and stars rating))\n",
    "        self.review_data = self.webpage.find_all(class_=[\"review-value\", \"review-rating-header\", \"star fill\"])\n",
    "        self.review_data = [i.text for i in self.review_data]\n",
    "        # Removing the first 20 elements as they are the totals.\n",
    "        self.review_data = self.review_data[20:]\n",
    "\n",
    "    # Below we are essentially grouping together all the filled stars, as the list had each star fill as an\n",
    "    # individual item, so we wanted to group them together to allow us to then get the last filled in star in the list,\n",
    "    #  being the overall rating for that element.\n",
    "    def getting_the_stars(self):\n",
    "        current_group = []\n",
    "        ready_to_group = []\n",
    "        for i, value in enumerate(self.review_data):\n",
    "            if len(self.review_data[i]) == 1:\n",
    "                current_group.append(int(self.review_data[i]))\n",
    "            else:\n",
    "                # Using a try clause as the first iteration wont work, (because of the [-1] index).\n",
    "                try:\n",
    "                    ready_to_group.append(current_group[-1])\n",
    "                    ready_to_group.append(self.review_data[i])\n",
    "                    current_group = [] # Resetting the that star cluster.\n",
    "                except:\n",
    "                    ready_to_group.append(current_group)\n",
    "                    ready_to_group.append(self.review_data[i])\n",
    "                    current_group = []\n",
    "\n",
    "        ready_to_group = [i for i in ready_to_group if i != []] # Removing the empty lists.\n",
    "        return ready_to_group\n",
    "    \n",
    "    # Putting every even index as a key and every odd index as a value in a dictionary\n",
    "    \n",
    "    def putting_into_df(self, data):\n",
    "        keys = [\"Aircraft\", \"Type Of Traveller\", \"Seat Type\", \"Route\", \"Date Flown\", \n",
    "                \"Seat Comfort\", \"Cabin Staff Service\", \"Food & Beverages\", \"Inflight Entertainment\", \n",
    "                \"Ground Service\", \"Wifi & Connectivity\", \"Value For Money\", \"Recommended\"]\n",
    "    # Data frame with the keys as the columns. We put in the keys prior \n",
    "    # as it was causing issues when we tried to append. NaN values were getting pushed to the bottom.\n",
    "        df = pd.DataFrame(columns=keys)\n",
    "        review_dict = {} # Temporary dictionary to hold the data for each review. \n",
    "\n",
    "        # Paring the data into a dictionary. If the key is \"Recommended\" then we want \n",
    "        # to put the dictionary into a dataframe (\"recommended\" is the last value in each review)\n",
    "        # this is so that we can have each row as each person's review, rather than it mashed together.\n",
    "\n",
    "        for i, val in enumerate(data):\n",
    "            # Getting every even index as the key and odd as the value. \n",
    "            if i % 2 == 0:\n",
    "                key = data[i]\n",
    "                value = data[i+1]\n",
    "                if key in review_dict:\n",
    "                    review_dict[key].append(value)\n",
    "                elif key not in review_dict:\n",
    "                    review_dict[key] = [value]\n",
    "            elif key == \"Recommended\":\n",
    "                # Putting the dictionary into a dataframe, using concat as append is deprecated.\n",
    "                df = pd.concat([df, pd.DataFrame(review_dict)], ignore_index=True)\n",
    "                review_dict = {}\n",
    "        return df\n",
    "\n",
    "    # Returning the dataframe, text and overall rating. \n",
    "    def get_reviews(self):\n",
    "        grouped_together = self.getting_the_stars()\n",
    "        stats_df = self.putting_into_df(grouped_together)\n",
    "        text_content = self.webpage.find_all(class_=\"text_content\")\n",
    "        # Getting the overall rating.\n",
    "        overall_rating = self.webpage.find_all(\"span\", attrs={\"itemprop\": \"ratingValue\"})\n",
    "        overall_rating = overall_rating[1:]\n",
    "        overall_rating = [i.text for i in overall_rating]\n",
    "        return stats_df, text_content, overall_rating\n",
    "\n",
    "scraped_df = pd.DataFrame()\n",
    "just_text = []\n",
    "overall_rating = []\n",
    "pages = 20 # Getting 20 pages as we start to get missing values after that. It should be enough. \n",
    "count = 0\n",
    "# Looping through the pages and scraping the data from each page.\n",
    "for page in range(1, pages+1):\n",
    "    webscrape = WebScraper(page)\n",
    "    df, reviews, rating = webscrape.get_reviews()\n",
    "    # Putting the output dataframe into a new variable, so then it can be reseted for the next page, which can then added as well etc. \n",
    "    scraped_df = pd.concat([scraped_df, df], ignore_index=True)\n",
    "    text_content = [i.text for i in reviews]\n",
    "    text_content = [i.replace(\"\\n\", \"\") for i in text_content]\n",
    "    just_text.append(text_content)\n",
    "    overall_rating.append(rating)\n",
    "    count += 1\n",
    "    print(f\"{count} pages scraped\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# The itertools.chain function is used to flatten the list of lists.\n",
    "removing_embed = list(itertools.chain(*just_text))\n",
    "scraped_df[\"Reviews\"] = removing_embed\n",
    "scraped_df[\"Trip Verified\"] = scraped_df[\"Reviews\"].apply(lambda x: True if \"Trip Verified\" in x or \"Verified Review\" in x else False)\n",
    "\n",
    "# Adding the overall rating to the dataframe. \n",
    "overall_rating = list(itertools.chain(*overall_rating))\n",
    "scraped_df[\"Overall Rating\"] = overall_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped_df.to_csv(\"BA_reviews.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
